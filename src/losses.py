# some parts of this code are adapted from https://github.com/xu-ji/information-bottleneck
import numpy as np
import torch
from torch.utils.data import DataLoader


def est_MI(model, dataset, sz, jensen, requires_grad=True):
    DEVICE = next(model.parameters()).device
    ii = np.random.choice(len(dataset), size=sz, replace=False)
    x = torch.stack([torch.tensor(dataset[i][0]) for i in ii], dim=0).to(DEVICE)

    if not requires_grad:
        model.eval()
        with torch.no_grad():
            z, log_prob = model(x, repr=True)
            log_marg_prob = model.log_marg_prob(z, x, jensen=jensen)
        model.train()
    else:
        z, log_prob = model(x, repr=True)
        log_marg_prob = model.log_marg_prob(z, x, jensen=jensen)

    return (log_prob - log_marg_prob).mean()  # (- H(C|X) + H(C))


def est_HC(model, dataset, sz, jensen=False):
    DEVICE = next(model.parameters()).device
    ii = np.random.choice(len(dataset), size=sz, replace=False)
    x = torch.stack([torch.tensor(dataset[i][0]) for i in ii], dim=0).to(DEVICE)
    enc = model.backbone(x) if model.backbone is not None else x
    sigma = model.pred_sigma(enc)
    H_c_up_to_const = torch.sum(torch.log(sigma + 1e-8))
    return H_c_up_to_const
    # z, log_prob = model(x, repr=True)
    # log_marg_prob = model.log_marg_prob(z, x, jensen=jensen)
    # return -log_marg_prob.mean()  # H(C)


def est_MI_cy(model, dataset, samples=500):
    # Expectation over joint (Y,C) of (log_prob(y | C) - log_prob(y))
    if model.training:
        model.eval()

    DEVICE = next(model.parameters()).device
    ii = np.random.choice(len(dataset), size=samples, replace=False)
    x = torch.stack([torch.tensor(dataset[i][0]) for i in ii], dim=0).to(DEVICE)
    ys = torch.stack([torch.tensor(dataset[i][-1]) for i in ii], dim=0).reshape(-1)

    with torch.no_grad():
        logits, _, C = model(x)
    probs = logits.softmax(dim=-1)
    MI = 0
    for p, y in zip(probs, ys):
        MI += torch.log(p[y]) - torch.log(torch.tensor(dataset.marg_y[y.item()]))

    return MI / samples


# https://github.com/artemyk/ibsgd/blob/1ccf656f87418ffc108d2469fdea4ae2b493d8b7/simplebinmi.py#L4
def get_unique_probs(x):
    _, _, unique_counts = np.unique(x, return_index=False, return_inverse=True, return_counts=True)
    return np.asarray(unique_counts / float(sum(unique_counts)))


# https://github.com/artemyk/ibsgd/blob/iclr2018/simplebinmi.py
def get_h(d, binsize):
    digitized = np.floor(d / binsize).astype('int')
    p_ts = get_unique_probs(digitized)
    return -np.sum(p_ts * np.log(p_ts))


def get_H_discretization(all_samples, num_bins):
    # kde = KernelDensity(kernel='gaussian').fit(all_samples.cpu().numpy())
    # log_dens = kde.score_samples(all_samples.cpu().numpy())
    # return -np.mean(log_dens)

    max_val = all_samples.max().item()
    min_val = all_samples.min().item()
    binsize = (max_val - min_val) / num_bins  # TODO: per component?
    feats_shifted = all_samples - min_val
    feats_np = feats_shifted.cpu().numpy()
    return get_h(feats_np, binsize)


def est_MI_binning(model, dataset, num_bins, batch_size):
    DEVICE = next(model.parameters()).device
    x = torch.stack([torch.tensor(dataset[i][0])
                     for i in range(len(dataset) // 2)], dim=0).to(DEVICE)  # shape (N, D)

    # shape (N,)
    ys = torch.stack([torch.tensor(dataset[i][-1]) for i in range(len(dataset) // 2)],
                     dim=0).reshape(-1)

    cs = torch.stack([torch.tensor(dataset[i][1]) for i in range(len(dataset) // 2)], dim=0)

    loader = DataLoader(x, shuffle=False, batch_size=batch_size)
    model.eval()  # turn off dropouts if any

    all_feats = []
    with torch.no_grad():
        for x_b in loader:
            # array of length num_layers, i-th element has a shape (N, layer_i_dim)
            feats = model.get_activations(x_b.to(DEVICE))
            if not all_feats:
                all_feats = [[] for _ in range(len(feats))]
            for i, feat in enumerate(feats):
                all_feats[i].append(feat)
    model.train()
    all_feats = [torch.cat(feats, dim=0) for feats in all_feats]

    # H(Z | X) = 0 | since Z is generated by X, thus
    # MI(Z; X) = H(Z) - H(Z | X) = H(Z)
    MI_XZ = []

    MI_ZY = []

    MI_ZC = []

    for feats in all_feats:
        max_val = feats.max().item()
        min_val = feats.min().item()
        binsize = (max_val - min_val) / num_bins

        feats_shifted = feats - min_val
        feats_np = feats_shifted.cpu().numpy()

        h_z = get_h(feats_np, binsize)

        MI_XZ.append(h_z)

        # Option one for MI(Z;Y)
        h_z_given_y = 0
        unique_ys = torch.unique(ys)
        for y in unique_ys:
            y_mask = (ys == y)
            feats_y = feats_np[y_mask.cpu().numpy()]
            p_y = dataset.marg_y[y.item()]  # y_mask.float().mean().item()
            h_z_y = get_h(feats_y, binsize)
            h_z_given_y += p_y * h_z_y

        # Calculate I(Z, Y) = H(Z) - H(Z|Y)
        mi_zy = h_z - h_z_given_y
        MI_ZY.append(mi_zy)

        h_z_given_c = 0
        unique_cs = range(dataset.num_concepts)
        for c in unique_cs:
            c_mask = (cs[:, c] == 1)
            feats_c = feats_np[c_mask.cpu().numpy()]
            p_c = dataset.marg_c[c]
            h_z_c = np.mean([get_h(feats_c[:, i], binsize) for i in range(len(feats_c[0]))])
            h_z_given_c += p_c * h_z_c / dataset.num_concepts
        mi_zc = h_z - h_z_given_c
        MI_ZC.append(mi_zc)

    return MI_XZ, MI_ZY, MI_ZC
